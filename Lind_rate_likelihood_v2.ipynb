{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperiax\n",
    "\n",
    "\n",
    "from hyperiax.tree.topology import symmetric_topology, asymmetric_topology, read_topology, write_topology\n",
    "from hyperiax.tree import HypTree\n",
    "from hyperiax.plotting import plot_tree\n",
    "\n",
    "from jax.random import PRNGKey, split\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from hyperiax.execution import OrderedExecutor\n",
    "from hyperiax.models import UpLambdaReducer, DownLambda, UpLambda\n",
    "from hyperiax.models.functional import pass_up\n",
    "from hyperiax.tree.topology import symmetric_topology, asymmetric_topology\n",
    "from hyperiax.tree import HypTree\n",
    "from hyperiax.plotting import plot_tree_text, plot_tree_2d_scatter\n",
    "from matplotlib import pyplot as plt\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate likelihood estimation - rough draft\n",
    "\n",
    "In this notebook, we setup a rough draft of rate estimation of discerete variables \n",
    "\n",
    "This notebook follows work of Sergei T. (_better to say it's Felsenstein's prunning algorithm:_)\n",
    "\n",
    "The outline of this notebook comes in; \n",
    "\n",
    "    1. Creating the tree\n",
    "\n",
    "    2. Simulating discerete character from root to leafs.  (NOT implemented, i just draft some randomly)\n",
    "\n",
    "    3. One up pass using likelihood to restimate the the rate\n",
    "    \n",
    "    4. Optimizer to converge for correct estimation of the rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating the Tree\n",
    "\n",
    "First, we initialize a tree. This creates a tree with a chosen topology. This topology is a \"stupid\" class, in the sense that it contains no data - and only serves as a representation of the data we intend to work on.\n",
    "\n",
    "Setting `height=3` and `degree=2`, gives us a tree with 4 layers, where each node has `2` children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_str = \"((A:2,B:0.3):0.1,(C:0.5,D:1):0.6);\"\n",
    "#topology = read_topology(tree_str)\n",
    "tree = read_topology(tree_str,return_topology=False,precompute_child_gathers=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ST: usually the tail of the tree (i.e. brach 7 in this case) is not used in Ln computations. I set it to 0: [0.1, 2.0, 0.3, 0.6, 0.5, 1.0, 0.0]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \n",
      " ┌─┴─┐\n",
      "       \n",
      "┌┴┐ ┌┴┐\n",
      "A B C D\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "other_tree = False\n",
    "if other_tree:\n",
    "   #topology = symmetric_topology(height=2, degree=2)\n",
    "   topology = asymmetric_topology(2)\n",
    "\n",
    "   tree = HypTree(topology, precompute_child_gathers=True)\n",
    "\n",
    "\n",
    "   # Propreties of the tree\n",
    "\n",
    "   # Branch/edge length, assumed to be constant one \n",
    "   tree.add_property('edge_length', shape=(1,))\n",
    "\n",
    "   tree.data[\"edge_length\"]  = jnp.array([1.0] * tree.size)\n",
    "   #tree.data[\"edge_length\"]  = jnp.array([0.1, 2.0, 0.3, 0.6, 0.5, 1.0, 0.0])\n",
    "\n",
    "   for i,node in enumerate(tree.iter_topology_bfs()):\n",
    "      node.name = str(i)\n",
    "\n",
    "\n",
    "# plot tree again \n",
    "plot_tree_text(tree)\n",
    "plot_tree(tree,inc_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simulating node values\n",
    "\n",
    "Could be a markov chain, I will just draft some random values for now\n",
    "\n",
    "I have kept the strcutre of the code how to do it proberly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert some correct thing here. \n",
    "\n",
    "#@jax.jit\n",
    "#def down(noise, edge_length,parent_value, **args):\n",
    "#    # Insert \n",
    "#    return {'value':value}\n",
    "\n",
    "# Define root variable for our simulation \n",
    "#tree.data['value'] = tree.data['value'].at[0].set([0,0])\n",
    "\n",
    "# Execute the simulation on tree\n",
    "#downmodel = DownLambda(down_fn=down)\n",
    "#exe = OrderedExecutor(downmodel)\n",
    "#exe.down(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "-----\n",
      "[[1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Draw random 0,1 in the size corresponding to the under here\n",
    "# Random sampler \n",
    "# Chacters storing \n",
    "tree.add_property('value', shape=(2,))\n",
    "tree.data['value'] = tree.data['value'].at[tree.is_leaf].set(jax.random.bernoulli(key, p=0.8, shape=(sum(tree.is_leaf),2)))\n",
    "\n",
    "# Print over discerete variables \n",
    "print(tree.data['value'])\n",
    "print('-----')\n",
    "print(tree.data['value'][tree.is_leaf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estimating the inner nodes\n",
    "\n",
    "Here we could mask our internal nodes \"value\" from our downsimulation, since we would only now the leaf values. \n",
    "We keep with this format, and therefore more our leaf data to another variable\n",
    "\n",
    "\n",
    "# Upward Pass Computation\n",
    "\n",
    "We estimate the upwards probability from the leafs to the root; \n",
    "\n",
    "$$\n",
    "p(parent)= p(t1)*p(t2) \n",
    "$$\n",
    "where $p(t1)=d1*exp(Q*t1)^T$, and t1 is the branch length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add_property('estimated_value', shape=(2,))\n",
    "leaf_data = tree.data['value'][tree.is_leaf]\n",
    "tree.data['estimated_value'] = tree.data['estimated_value'].at[tree.is_leaf].set(leaf_data)\n",
    "\n",
    "leaf_edgelength = tree.data['edge_length'][tree.is_leaf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate the probability in the leaf nodes, since we only have data in chacaters in the leafs, and upwards we multiply the edgelengths with probabilies (but I may have misunderstood that part)\n",
    "\n",
    "Manual estimating here:\n",
    "\n",
    "_ST: it should be matrix exponential jax.scipy.linalg.expm() istead of element-wise exponential_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rate matrix, which is 2x2 as a function of input of alpha and beta\n",
    "# as a function \n",
    "Q_rate_matrix = lambda alpha, beta: jnp.array([[-alpha, alpha], [beta,-beta]])\n",
    "\n",
    "prob_estimation = lambda chacter,length : jnp.dot(chacter, jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*length))\n",
    "\n",
    "\n",
    "\n",
    "# Define alpha and beta\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperiax implementation of up\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_function(value,child_estimated_value, child_edge_length,leaf_mask, **kwargs):\n",
    "    # Do probability estimation \n",
    "    probs = jax.vmap(prob_estimation, in_axes=1)(child_estimated_value,child_edge_length)\n",
    "    prods = jnp.prod(probs, axis=0)\n",
    "\n",
    "   # Rescale the likelihood\n",
    "    result = prods / jnp.sum(prods, axis=1, keepdims=True)\n",
    "\n",
    "    # Mask padding - not the most efficiant, but problem with the reducers atm\n",
    "    result = jnp.where(leaf_mask[:,None], value, result) \n",
    "    return {'estimated_value': result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upmodel = UpLambda(up_fn=probability_function)\n",
    "upmodelexe = OrderedExecutor(upmodel)\n",
    "res = upmodelexe.up(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8664692  0.13353083]\n",
      " [0.76361114 0.23638888]\n",
      " [0.85846627 0.14153373]\n",
      " [1.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]]\n",
      "-----\n",
      "[[1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tree.data['estimated_value'])\n",
    "print('-----')\n",
    "print(tree.data['estimated_value'][tree.is_leaf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS: Manual of assymteric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6839397  0.31606027]\n",
      "[0.7396746  0.26032534]\n"
     ]
    }
   ],
   "source": [
    "n3 = jnp.dot(leaf_data[0],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "n4 = jnp.dot(leaf_data[1],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "# conditional Ln of the ancestor 3 and 4\n",
    "n3_4=n3*n4 # conditional Likelihoods \n",
    "SUM_n3_4=jnp.sum(n3_4) # Sum of likelihoods for normalization (scaling factor)\n",
    "SCALED_n3_4=n3_4/SUM_n3_4 # Normalized likelihoods at the node\n",
    "print(SCALED_n3_4)\n",
    "\n",
    "\n",
    "\n",
    "root=jnp.dot(SCALED_n3_4, jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1)) * jnp.dot(leaf_data[2],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "\n",
    "\n",
    "SUM_root=jnp.sum(root)\n",
    "SCALED_root=root/SUM_root\n",
    "print(SCALED_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _ST: manual computation of likelihod with rescaling_\n",
    "\n",
    "The same as before but now we use the rescaling trick to avoid underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6839397  0.31606027]\n",
      "[0.8240271  0.17597288]\n"
     ]
    }
   ],
   "source": [
    "#-- now we the rescaling trick\n",
    "\n",
    "n3 = jnp.dot(leaf_data[0],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "n4 = jnp.dot(leaf_data[1],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "# conditional Ln of the ancestor 3 and 4\n",
    "n3_4=n3*n4 # conditional Likelihoods \n",
    "SUM_n3_4=jnp.sum(n3_4) # Sum of likelihoods for normalization (scaling factor)\n",
    "SCALED_n3_4=n3_4/SUM_n3_4 # Normalized likelihoods at the node\n",
    "print(SCALED_n3_4)\n",
    "\n",
    "n5 = jnp.dot(leaf_data[2],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "n6 = jnp.dot(leaf_data[3],jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "\n",
    "\n",
    "# conditional Ln of the ancestor 5 and 6\n",
    "n5_6=n5*n6\n",
    "\n",
    "SUM_n5_6=jnp.sum(n5_6)\n",
    "SCALED_n5_6=n5_6/SUM_n5_6\n",
    "print(SCALED_n5_6)\n",
    "# conditional Ln at the root\n",
    "root=jnp.dot(SCALED_n3_4, jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1)) * jnp.dot(SCALED_n5_6, jax.scipy.linalg.expm(Q_rate_matrix(alpha,beta)*1))\n",
    "SUM_root=jnp.sum(root)\n",
    "SCALED_root=root/SUM_root\n",
    "\n",
    "# Calculate the log-likelihood of the tree\n",
    "root_pi=jnp.array([0.5, 0.5])\n",
    "log_likelihood_sum = jnp.sum(jnp.log(jnp.array([SUM_n3_4, SUM_n5_6, SUM_root])))  # Sum of logs of scaling factors\n",
    "loglik = log_likelihood_sum + jnp.log(jnp.sum(jnp.exp(jnp.log(root_pi) + jnp.log(SCALED_root))))\n",
    "# this is the final Ln\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do the optimization to estimate alpha/beta\n",
    "\n",
    "NOTE - I may not have recalled the likelihood function correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: -0.8132617473602295\n",
      "Iteration 10, Loss: -0.8635314106941223\n",
      "Iteration 20, Loss: -0.9223139882087708\n",
      "Iteration 30, Loss: -0.9899564981460571\n",
      "Iteration 40, Loss: -1.066314697265625\n",
      "Iteration 50, Loss: -1.1507490873336792\n",
      "Iteration 60, Loss: -1.2422735691070557\n",
      "Iteration 70, Loss: -1.3397363424301147\n",
      "Iteration 80, Loss: -1.441970944404602\n",
      "Iteration 90, Loss: -1.5478955507278442\n",
      "Estimated alpha: 1.6179996728897095\n",
      "Estimated beta: 1.6179994344711304\n",
      "Final negative log-likelihood: -1.6565672159194946\n"
     ]
    }
   ],
   "source": [
    "# Use JAX's BFGS optimizer from scipy.optimize\n",
    "\n",
    "from jax.example_libraries import optimizers\n",
    "from jax import grad\n",
    "pi_root = [1,1]\n",
    "pred_root = tree.data[\"estimated_value\"][tree.is_root]\n",
    "\n",
    "# Initialize optimization parameters\n",
    "initial_params = jnp.array([0.5, 0.5])\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "def negative_log_likelihood(params):\n",
    "    alpha, beta = params\n",
    "    Q = Q_rate_matrix(alpha, beta)\n",
    "    # note we do not include edge length since we are in the root\n",
    "    root_probs = jnp.exp(Q)\n",
    "    likelihood = jnp.sum(jnp.array(pi_root) * jnp.dot(pred_root, root_probs))\n",
    "\n",
    "    return -jnp.log(likelihood)\n",
    "\n",
    "\n",
    "# Define the optimization function\n",
    "@jax.jit\n",
    "def optimize_step(i, opt_state, pi_root, pred_root):\n",
    "    params = get_params(opt_state)\n",
    "    value, grads = jax.value_and_grad(negative_log_likelihood)(params)\n",
    "    return opt_update(i, grads, opt_state), value\n",
    "\n",
    "# Initialize the optimizer\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=0.01)\n",
    "opt_state = opt_init(initial_params)\n",
    "\n",
    "# Optimization loop\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    # Re-estimate the root value using upmodelexe.up\n",
    "    upmodelexe.up(tree)\n",
    "    pred_root = jnp.array(tree.data[\"estimated_value\"][tree.is_root])\n",
    "    \n",
    "    opt_state, value = optimize_step(i, opt_state, jnp.array(pi_root), pred_root)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {value}\")\n",
    "\n",
    "# Get the final optimized parameters\n",
    "final_params = get_params(opt_state)\n",
    "estimated_alpha, estimated_beta = final_params\n",
    "\n",
    "print(f\"Estimated alpha: {estimated_alpha}\")\n",
    "print(f\"Estimated beta: {estimated_beta}\")\n",
    "print(f\"Final negative log-likelihood: {negative_log_likelihood(final_params)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
