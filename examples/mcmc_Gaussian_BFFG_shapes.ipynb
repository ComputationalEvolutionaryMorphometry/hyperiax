{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC using backward filtering, forward guiding for shapes\n",
    "\n",
    "Parameter inference for trees with Gaussian transitions along edges and observations at the leaf nodes for landmark represented shapes. Please refer to the notebook [mcmc_Gaussian_BFFG.ipynb](mcmc_Gaussian_BFFG.ipynb) for a simpler version with $\\mathbb R^2$ data. In the present version, the node covariance is constant throughout the tree similarly to [mcmc_Gaussian_BFFG.ipynb](mcmc_Gaussian_BFFG.ipynb). Shape dependent node covariance will follow in a later version.\n",
    "\n",
    "The conditioning and upwards/downwards message passing and fusing operations follow the backward filtering, forward guiding approach of Frank van der Meulen, Moritz Schauer et al., see https://arxiv.org/abs/2010.03509 and https://arxiv.org/abs/2203.04155 . The latter reference provides an accesible introduction to the scheme and the notation used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey, split\n",
    "import hyperiax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from hyperiax.execution import LevelwiseTreeExecutor\n",
    "from hyperiax.models import DownLambda, UpDownLambda\n",
    "from hyperiax.models.functional import sum_fuse_children\n",
    "from hyperiax.tree.updaters import update_noise_inplace\n",
    "from hyperiax.mcmc import ParameterStore, VarianceParameter\n",
    "from hyperiax.mcmc.metropolis_hastings import metropolis_hastings\n",
    "from hyperiax.mcmc.plotting import trace_plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed,\n",
    "seed = 423\n",
    "#import os; seed = int(os.urandom(5).hex(), 16)\n",
    "key = PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape related setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# plotting\n",
    "def plot_shape(q):\n",
    "    q = q.reshape((-1,d))\n",
    "    plt.plot(q[:,0],q[:,1],'.')\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shape and plot\n",
    "d = 2; n = 18 # d = dimension of embedding space (usually 2), n = number of landmarkds\n",
    "phis = jnp.linspace(0,2*jnp.pi,n,endpoint=False) # circular shape\n",
    "root = jnp.vstack((jnp.cos(phis),jnp.sin(phis))).T.flatten()\n",
    "\n",
    "# plot\n",
    "plot_shape(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion and covariance specification for shape processes\n",
    "kQ12 = lambda x,params: params['k_alpha']*jnp.exp(-.5/params['k_sigma']*jnp.sum(jnp.square(x),2))\n",
    "\n",
    "# evaluate k on two pairs of landmark configurations\n",
    "kQ12_q = lambda q1,q2,params: kQ12(q1.reshape((-1,d))[:,jnp.newaxis,:]-q2.reshape((-1,d))[jnp.newaxis,:,:],params)\n",
    "\n",
    "# evaluate k on one landmark configurations against itself with each landmark pair resulting in a dxd matric\n",
    "# i,jth entry of result is kQ12(x_i,x_j)*eye(d)\n",
    "def Q12(q,params): \n",
    "    A = jnp.einsum('ij,kl->ikjl',kQ12_q(q,q,params),jnp.eye(2))\n",
    "    return A.reshape((A.shape[0]*A.shape[1],A.shape[2]*A.shape[3]))\n",
    "\n",
    "# diffusion matrix\n",
    "sigma = lambda params: Q12(root,params)\n",
    "# covariance matrix\n",
    "def a(params): _sigma = sigma(params); return jnp.einsum('ij,kj->ik',_sigma,_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian tree, constant node covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the tree. We set the root to the shape defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tree and initialize with noise\n",
    "tree = hyperiax.tree.builders.symmetric_tree(1,20)\n",
    "print('Tree:',tree)\n",
    "\n",
    "# set edge lengths on all nodes\n",
    "edge_length = 1.\n",
    "tree['edge_length'] = edge_length\n",
    "\n",
    "# data dimension\n",
    "d = 2\n",
    "\n",
    "# root value\n",
    "tree.root['value'] = root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define parameters for the Gaussian transition kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters, variance and observation noise\n",
    "params = ParameterStore({\n",
    "    'k_alpha': VarianceParameter(.1), # kernel amplitude, governs global tree variance\n",
    "    'k_sigma': VarianceParameter(.25), # kernel width, for Gaussian kernels this is proportional to the variance\n",
    "    'obs_var': VarianceParameter(1e-3) # observation noise variance\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follows the down transitions. At first, we define the unconditional transitions, which are just Gaussian samples. The covariance is sqare of the pairwise kernel evaluations in `sigma(params)` times the variance parameter times edge lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmapped version of down_unconditional. In this version, the function takes a single node, not a batch (contrast to the batched version in mcmc_Gaussian_BFFG.ipynb)\n",
    "@jax.jit\n",
    "def down_unconditional(noise,edge_length,parent_value,params,**args):\n",
    "    def f(noise,edge_length,parent_value):\n",
    "        var = edge_length # variance is edge length\n",
    "        return {'value': parent_value+jnp.sqrt(var)*jnp.einsum('ij,j->i',sigma(params),noise)}\n",
    "\n",
    "    return jax.vmap(f)(noise,edge_length,parent_value)\n",
    "downmodel_unconditional = DownLambda(down_fn=down_unconditional)\n",
    "down_unconditional = LevelwiseTreeExecutor(downmodel_unconditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now draw noise and perform a downwards pass. This gives values at all nodes of the tree. Note that observation noise is not added to the leaves yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subkey, key = split(key)\n",
    "noise_tree = hyperiax.tree.initializers.initialize_noise(tree, subkey, (n*d,))\n",
    "dtree = down_unconditional.down(noise_tree,params.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add uncorrelated observation noise to leaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the sampled tree and add noise to leaf nodes\n",
    "leaf_tree = dtree.copy()\n",
    "for node in leaf_tree.iter_leaves():\n",
    "    key, subkey = split(key)\n",
    "    node['value'] += jnp.sqrt(params['obs_var'].value)*jax.random.normal(subkey,node['value'].shape) # add observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a generated tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "for i in range(n):\n",
    "    dtree.plot_tree_2d(selector=lambda z: z['value'].reshape((n,d))[i],ax=ax)\n",
    "plt.gca().set_title('Sampled tree without leaf noise')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the backwards filter through the up function. The Gaussian are parametrized in the $(c,F,H)$ format make the fuse just a sum of the results of the up operation. See https://arxiv.org/abs/2203.04155 for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backwards filter\n",
    "@jax.jit\n",
    "def up(noise,edge_length,F_T,H_T,params,**args):\n",
    "    def f(noise,edge_length,F_T,H_T):\n",
    "        var = edge_length # variance is edge length\n",
    "        covar = var*a(params) # covariance matrix\n",
    "\n",
    "        Sigma_T = jnp.linalg.inv(H_T)\n",
    "        v_T = Sigma_T@F_T\n",
    "    \n",
    "        invPhi_0 = (jnp.eye(n*d)+H_T*covar)\n",
    "        Sigma_0 = Sigma_T@invPhi_0 # = Sigma_T+covar\n",
    "        H_0 = jnp.linalg.inv(Sigma_0)\n",
    "        F_0 = jnp.linalg.solve(invPhi_0,F_T)\n",
    "        v_0 = Sigma_0@F_0\n",
    "        c_0 = -jax.scipy.stats.multivariate_normal.logpdf(v_0,jnp.zeros(n*d),Sigma_0)\n",
    "    \n",
    "        return {'c_0': c_0, 'F_0': F_0, 'H_0': H_0, 'F_T': F_T, 'H_T': H_T}\n",
    "    return jax.vmap(f)(noise,edge_length,F_T,H_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the tree for up by computing the $c,F,H$-values at the leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tree for up\n",
    "def init_up(tree,params):\n",
    "    for node in tree.iter_bfs():\n",
    "        if node.children and node.parent:\n",
    "            del node.data['value']\n",
    "        else:\n",
    "            v = node['value']\n",
    "            Sigma = params['obs_var'].value*jnp.eye(n*d)\n",
    "            H = jnp.eye(n*d)/params['obs_var'].value\n",
    "            F = H@v\n",
    "            c = -jax.scipy.stats.multivariate_normal.logpdf(v,jnp.zeros(n*d),Sigma)\n",
    "            node.data = {**node.data, 'F_T': F, 'H_T': H}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the conditional downwards pass, i.e. the forwards guiding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def down_conditional(noise,edge_length,F_T,H_T,parent_value,params,**args):\n",
    "    def f(noise,edge_length,F_T,H_T,parent_value):\n",
    "        x = parent_value\n",
    "        var = edge_length # variance is edge length\n",
    "        covar = var*a(params) # covariance matrix\n",
    "\n",
    "        invSigma = jnp.linalg.inv(covar)\n",
    "        H = H_T+invSigma\n",
    "        mu = jnp.linalg.solve(H,F_T+invSigma@x)\n",
    "        #return {'value': mu+jnp.linalg.solve(jnp.real(jax.scipy.linalg.sqrtm(H)),noise)}\n",
    "        return {'value': mu+jax.scipy.linalg.solve_triangular(jax.scipy.linalg.cholesky(H,lower=True),noise)}\n",
    "\n",
    "    return jax.vmap(f)(noise,edge_length,F_T,H_T,parent_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model and executor for the backwards filter (up) and forwards guiding (down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and executor\n",
    "updownmodel = UpDownLambda(up_fn=up,fuse_fn=sum_fuse_children(axis=0),down_fn=down_conditional)\n",
    "updown = LevelwiseTreeExecutor(updownmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make an upwards pass and a downwards conditional sampling to test. Subsequently, we time the three operations (uncondtional down, conditional down, and up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backwards filter and fowards guiding\n",
    "utree = leaf_tree.copy()\n",
    "init_up(utree,params)\n",
    "utree = updown.up(utree,params.values())\n",
    "utree.root['value'] = root\n",
    "dtree_conditional = updown.down(utree,params.values())\n",
    "\n",
    "# time the operations\n",
    "subkey, key = split(key)\n",
    "noise_tree = hyperiax.tree.initializers.initialize_noise(tree, subkey, (n*d,))\n",
    "%time down_unconditional.down(noise_tree,params.values())\n",
    "%time updown.up(utree,params.values())\n",
    "updown.up(utree,params.values())\n",
    "%time updown.up(utree,params.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the setup by sampling a number of trees and computing mean and covariance of the leaf data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do statistics on the leaf values\n",
    "leaves = jnp.array([n['value'] for n in dtree.iter_leaves()])\n",
    "c = utree.root['c_0']; F = utree.root['F_0']; H = utree.root['H_0']\n",
    "print(\"root conditional mean vs. sample mean:\",jnp.linalg.solve(H,F),jnp.mean(leaves,0))\n",
    "print(\"root conditional cov vs. sample cov:\",jnp.linalg.inv(H),jnp.cov(leaves.T))\n",
    "\n",
    "# sample statistics\n",
    "K = 500 # number samples\n",
    "samples = jnp.zeros((K,len(list(dtree.iter_leaves())),n*d))\n",
    "\n",
    "# sample new noise\n",
    "update_noise = lambda tree,key: update_noise_inplace(lambda node,new: new,tree,key)\n",
    "\n",
    "for i in tqdm(range(K)):\n",
    "    subkey, key = split(key)\n",
    "    update_noise(utree,subkey)\n",
    "    dtree = updown.down(utree,params.values())\n",
    "    # collect values\n",
    "    samples = samples.at[i].set(jnp.array([n.data['value']-m.data['value'] for n,m in zip(dtree.iter_leaves(),leaf_tree.iter_leaves())]))\n",
    "print(\"observation noise: \")\n",
    "print(\"mean: \",jnp.mean(samples,axis=(0,1)))\n",
    "print(\"cov: \",jnp.cov(samples.reshape(-1,n*d).T))\n",
    "for i in tqdm(range(K)):\n",
    "    subkey, key = split(key)\n",
    "    update_noise(noise_tree,subkey)\n",
    "    dtree = down_unconditional.down(noise_tree,params.values())\n",
    "    # collect values\n",
    "    samples = samples.at[i].set(jnp.array([n.data['value'] for n in dtree.iter_leaves()]))\n",
    "    # add observation noise\n",
    "    subkey,key = jax.random.split(key)\n",
    "    samples = samples.at[i].set(samples[i]+jnp.sqrt(params['obs_var'].value)*jax.random.normal(subkey,samples[i].shape))\n",
    " print sample statistics for all leaves\n",
    "print(\"leaves: \")\n",
    "for i in range(samples.shape[1]):\n",
    "    print(\"mean: \",jnp.mean(samples[:,i],0))\n",
    "    print(\"cov: \",jnp.cov(samples[:,i].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the inverse gamma log PDF function\n",
    "def inverse_gamma_logpdf(x, alpha, beta):\n",
    "    return alpha * jnp.log(beta) - jax.scipy.special.gammaln(alpha) - (alpha + 1) * jnp.log(x) - beta / x\n",
    "\n",
    "# Set the parameters for the inverse gamma distribution\n",
    "alpha = 2\n",
    "beta = 0.003\n",
    "\n",
    "# Generate values in the interval from 0 to 0.01\n",
    "x_values = jnp.linspace(0.0001, 0.01, 100)  # Avoid zero to prevent log(0)\n",
    "y_values = inverse_gamma_logpdf(x_values, alpha, beta)\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_values, y_values, label=f'Inverse Gamma Log PDF (alpha={alpha}, beta={beta})')\n",
    "plt.title('Inverse Gamma Log PDF')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Log PDF')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two MCMC runs: First, we use that the model with constant node covariance is fully Gaussian and we can read the data likelihood directly from the results of upwards pass. Subsequently, we also sample the state of the tree to get a likelihood approximation from the conditional downwards pass. This version is not necessary in the current model, but it points towards how inference in non-Gaussian models (e.g. non-linear diffusion processes along the edges) will look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference for Gaussian model, likelihood from backwards filtering\n",
    "def log_likelihood(state):\n",
    "    \"\"\"Log likelihood of the tree.\"\"\"\n",
    "    parameters,tree = state\n",
    "    v,c,F,H = tree.root['value'],tree.root['c_0'],tree.root['F_0'],tree.root['H_0']\n",
    "    return -c+F@v-.5*v.T@H@v\n",
    "\n",
    "def log_posterior(data,state):\n",
    "    \"\"\"Log posterior given the state and data.\"\"\"\n",
    "    parameters,_ = state\n",
    "    log_prior = parameters.log_prior()\n",
    "    log_like = log_likelihood(state)\n",
    "    return log_prior + log_like\n",
    "\n",
    "def proposal(data, state, key):\n",
    "    parameters,tree = state\n",
    "\n",
    "    new_parameters = parameters.propose(key)\n",
    "    # backwards filtering with updated parameters\n",
    "    utree = data.copy(); init_up(utree,new_parameters)\n",
    "    utree = updown.up(utree,new_parameters.values())\n",
    "\n",
    "    return new_parameters,utree\n",
    "\n",
    "# tree values and parameters\n",
    "init_params = ParameterStore({\n",
    "    'k_alpha': VarianceParameter(.25), # kernel amplitude, governs global tree variance\n",
    "    'k_sigma': VarianceParameter(.5), # kernel width, for Gaussian kernels this is proportional to the variance\n",
    "    'obs_var': VarianceParameter(1e-3,alpha=2,beta=.003,keep_constant=False) # observation noise variance. We keep it constant here because of lacking identifiability (which will not be the case with higher-dimensional correlated data)\n",
    "    })\n",
    "print(\"Initial parameters: \",init_params.values())\n",
    "print(\"data parameters: \",params.values())\n",
    "\n",
    "# initial state\n",
    "leaf_tree.root['value'] = root\n",
    "utree = leaf_tree.copy(); init_up(utree,init_params)\n",
    "init_state = (init_params,updown.up(utree,init_params.values()))\n",
    "\n",
    "# Run Metropolis-Hastings\n",
    "subkey, key = split(key)\n",
    "log_likelihoods,samples = metropolis_hastings(log_posterior, proposal, leaf_tree, init_state, 200, burn_in=200, rng_key=key, savef=lambda state: state[0])\n",
    "\n",
    "# plot\n",
    "plt.plot(log_likelihoods)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title('Log likelihood')\n",
    "trace_plots(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference for Gaussian model, likelihood from forward guiding\n",
    "\n",
    "# Crank-Nicolson update with possibly node-dependent lambd\n",
    "lambd = lambda node: .9\n",
    "update_CN = lambda tree,key: update_noise_inplace(lambda node,new: node['noise']*lambd(node)+jnp.sqrt((1-lambd(node)**2))*new,tree,key)\n",
    "zero_noise = lambda tree,key: update_noise_inplace(lambda node,new: jnp.zeros_like(node['noise']),tree,key)\n",
    "\n",
    "# downwards pass to compute likelihoods\n",
    "#@jax.jit\n",
    "def down_log_likelihood(noise,value,edge_length,parent_value,params,**args):\n",
    "    var = edge_length # variance is edge length\n",
    "    #covar = jnp.einsum('i,jk->ijk',var,a(params)) # covariance matrix\n",
    "    covar = a(params)/params['k_alpha']**2 # covariance without amplitude\n",
    "    sqrt_covar = sigma(params)/params['k_alpha'] # square root covariance without amplitude\n",
    "    #sqrt_covar = jnp.real(jax.scipy.linalg.sqrtm(covar))\n",
    "    #chol_covar = jax.scipy.linalg.cholesky(covar,lower=True)\n",
    "\n",
    "    #return {'log_likelihood': jax.vmap(lambda value,m,covar: jax.scipy.stats.multivariate_normal.logpdf(value,m,covar))(value,parent_value,covar)}\n",
    "    #return {'log_likelihood': jnp.mean(jax.scipy.stats.norm.logpdf(noise),1) }\n",
    "    #return {'log_likelihood': jnp.mean(jax.scipy.stats.norm.logpdf(\n",
    "    #    jax.vmap(lambda v,m,covar: jnp.linalg.solve(jnp.real(jax.scipy.linalg.sqrtm(covar)),v-m))(value,parent_value,covar)\n",
    "    #    ),1)\n",
    "    #    }\n",
    "    #return {'log_likelihood': jnp.mean(jax.scipy.stats.norm.logpdf(\n",
    "    #    jax.vmap(lambda v,m,covar: jax.scipy.linalg.solve_triangular(jax.scipy.linalg.cholesky(covar,lower=True),v-m))(value,parent_value,covar)\n",
    "    #    ),1)\n",
    "    #    }\n",
    "    #return {'log_likelihood': jnp.mean(\n",
    "    #    jax.vmap(lambda v,m,var: jax.scipy.stats.norm.logpdf(jax.scipy.linalg.solve_triangular(chol_covar,v-m),0,jnp.sqrt(var)*params['k_alpha']))(value,parent_value,var)\n",
    "    #    ,1)\n",
    "    #    }\n",
    "    #diffs = jax.vmap(lambda v,m,var: v-m)(value,parent_value,var)\n",
    "    #anoise = jax.vmap(lambda v,m,var: jnp.linalg.solve(jnp.sqrt(var)*sqrt_covar,v-m))(value,parent_value,var)\n",
    "    #print('z',anoise.shape,jnp.mean(anoise,0),jnp.diag(jnp.cov(anoise.T)))\n",
    "    #print('z',anoise.shape,jnp.diag(jnp.cov(anoise.T)))\n",
    "    return {'log_likelihood': jnp.mean(\n",
    "        jax.vmap(lambda v,m,var: jax.scipy.stats.norm.logpdf(jnp.linalg.solve(sqrt_covar,v-m),0,jnp.sqrt(var)*params['k_alpha']))(value,parent_value,var)\n",
    "        ,1)\n",
    "        }\n",
    "downmodel_log_likelihood = DownLambda(down_fn=down_log_likelihood)\n",
    "down_log_likelihood = LevelwiseTreeExecutor(downmodel_log_likelihood,batch_size=100)\n",
    "\n",
    "# log likelihood of the tree\n",
    "def log_likelihood(data,state):\n",
    "    \"\"\"Log likelihood of the tree.\"\"\"\n",
    "    params,tree = state\n",
    "    log_likelihood_tree = down_log_likelihood.down(tree,params.values())\n",
    "    log_likelihood_tree.root['log_likelihood'] = 0\n",
    "    tree_log_likelihood = jnp.mean(jnp.array([node['log_likelihood'] for node in log_likelihood_tree.iter_bfs()]))\n",
    "    residuals = jnp.array([sample['value']-obs['value'] for sample,obs in zip(tree.iter_leaves(),data.iter_leaves())]) \n",
    "    leaves_log_likelihood = jnp.mean(jax.scipy.stats.norm.logpdf(residuals,0,jnp.sqrt(params['obs_var'].value))\n",
    "    return tree_log_likelihood+leaves_log_likelihood\n",
    "\n",
    "def log_posterior(data,state):\n",
    "    \"\"\"Log posterior given the state and data.\"\"\"\n",
    "    parameters,_ = state\n",
    "    log_prior = parameters.log_prior()\n",
    "    log_like = log_likelihood(data,state)\n",
    "    return log_prior + log_like\n",
    "\n",
    "def proposal(data, state, key):\n",
    "    subkeys = jax.random.split(key,2)\n",
    "    parameters,tree = state\n",
    "\n",
    "    # new tree with the leaf data\n",
    "    utree = tree.copy(); \n",
    "    for data_leaf,utree_leaf in zip(data.iter_leaves(),utree.iter_leaves()):\n",
    "        utree_leaf['value'] = data_leaf['value']\n",
    "\n",
    "    # update parameters\n",
    "    new_parameters = parameters.propose(subkeys[0])\n",
    "    # backwards filtering with updated parameters\n",
    "    init_up(utree,new_parameters)\n",
    "    utree = updown.up(utree,new_parameters.values())\n",
    "\n",
    "    # update tree, CN update and forward filtering with the updated noise\n",
    "    utree_CN = update_CN(utree,subkeys[1])\n",
    "    dtree = updown.down(utree_CN,new_parameters.values())\n",
    "\n",
    "    return new_parameters,dtree\n",
    "\n",
    "# tree values and parameters\n",
    "init_params = ParameterStore({\n",
    "    'k_alpha': VarianceParameter(.25,alpha=3,beta=.5,keep_constant=False), # kernel amplitude, governs global tree variance\n",
    "    'k_sigma': VarianceParameter(.5,keep_constant=False), # kernel width, for Gaussian kernels this is proportional to the variance\n",
    "    'obs_var': VarianceParameter(1e-3,alpha=2,beta=.003,keep_constant=False) # observation noise variance. We keep it constant here because of lacking identifiability (which will not be the case with higher-dimensional correlated data)\n",
    "    })\n",
    "print(\"Initial parameters: \",init_params.values())\n",
    "print(\"data parameters: \",params.values())\n",
    "\n",
    "# initial state\n",
    "leaf_tree.root['value'] = root\n",
    "leaf_tree = zero_noise(leaf_tree,key)\n",
    "utree = leaf_tree.copy(); init_up(utree,init_params);\n",
    "init_state = (init_params,updown.down(updown.up(utree,init_params.values()),init_params.values()))\n",
    "\n",
    "# Run Metropolis-Hastings\n",
    "subkey, key = split(key)\n",
    "log_likelihoods, samples = metropolis_hastings(log_posterior, proposal, leaf_tree, init_state, 200, burn_in=200, rng_key=key, savef=lambda state: state[0])\n",
    "\n",
    "# plot\n",
    "plt.plot(log_likelihoods)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title('Log likelihood')\n",
    "trace_plots(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
