{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local update in Gaussian models\n",
    "\n",
    "Let $n$ be a node with parent $p$ and children $c_1,\\ldots,c_k$. Let $l_{p,n}$ denote the edge length from $p$ to $n$. Let $v_n$ be the value of $n$. We assume that $v_n|v_p\\simeq \\mathcal N(v_p,l_{p,n}\\Sigma(v_p,\\theta_p))$ where $\\Sigma(v_p)$ is the covariance matrix, a function of the node value $v_p$ and parameters $\\theta_p$.\n",
    "We wish to sample from the distribution $v_n|v_p,v_{c_1},\\ldots v_{c_n}$. We have\n",
    "$$p(v_n|v_p,c_1...,c_n) = \\frac{p(v_{c_1},\\ldots,v_{c_n}|v_p,v_n)p(v_n|v_p)}{p(v_{c_1},\\ldots,v_{c_n}|v_p)}= \\frac{p(v_{c_1}|v_n)\\cdots p(v_{c_n}|v_n)p(v_n|v_p)}{p(v_{c_1},\\ldots,v_{c_n}|v_p)}$$\n",
    "using that $v_{c_1}|v_n,\\ldots,v_{c_n}|v_n$ are independent.\n",
    "All parent conditional probabilities are Gaussian, and we write the densities on canonical form for exponential families, i.e. $p(v_n|v_p) = \\exp(-c_p+F_p^Tv_n-\\frac12 v_n^TH_pv_n)$ (see https://arxiv.org/abs/2203.04155). Note that $c=-\\log p(v;0,\\Sigma)$, $F=Hv$ and $H=\\Sigma^{-1}$. Write $H_{c_i}=l_{n,c_i}^{-1}\\Sigma(v_n,\\theta_n)^{-1}$. In this notation, we have\n",
    "$$\\log p(v_{c_i}|v_n)\n",
    "= \\mathrm{constant}-\\frac12(v_{c_i}-v_n)^Tl_{n,c_i}^{-1}\\Sigma(v_n,\\theta_n)^{-1}(v_{c_i}-v_n)\n",
    "= -c_{c_i}+F_{c_i}^Tv_n-\\frac12 v_n^TH_{c_i}v_n\n",
    "$$\n",
    "and thus\n",
    "\\begin{align}\n",
    "& \\log p(v_n|v_p,v_{c_1},\\ldots,v_{c_n}) \\\\\n",
    "& = -\\big(c_p+\\sum_{i=1}^n c_{c_i}\\big)+\\big(F_p+\\sum_{i=1}^n F_{c_i}^T\\big)v_n-\\frac12 v_n^T\\big(H_p+\\sum_{i=1}^n H_{c_i}\\big)v_n\n",
    "-\\log p(v_{c_1},\\ldots,v_{c_n}|v_p)\n",
    "\\ .\n",
    "\\end{align}\n",
    "We don’t need to wory about $\\log p(v_{c_1},\\ldots,v_{c_n}|v_p)$, since it cancels out in the MH step.\n",
    "In the MH step of the MCMC sampler, we sample a new value and parameters $v_n’,\\theta_n’$ based on $v_n,\\theta_n$ and accept/reject it by evaluating the log-ratio\n",
    "$$\\log p(v_n’|v_p,v_{c_1},\\ldots,v_{c_n})-\\log p(v_n|v_p,v_{c_1},\\ldots,v_{c_n})$$\n",
    "According to the above, to evaluate this we need $\\sum_{i=1}^n c_{c_i}$, $\\sum_{i=1}^n F_{c_i}^T$, and $\\sum_{i=1}^n H_{c_i}$ from the up operation with $H_{c_i}=l_{n,c_i}^{-1}\\Sigma(v_n,\\theta_n)^{-1}$, $F_{c_i}=H_{c_i}v_{c_i}$, and $c_{c_i}=-\\log \\phi(v_{c_i};0,l_{n,c_i}\\Sigma(v_n,\\theta_n))$ where $\\phi(x;0,\\Sigma)$ is the Gaussian density.\n",
    "This gives the operations\n",
    "#\n",
    "- Up operation:\n",
    "  - Compute $H_{c_i}=l_{n,c_i}^{-1}\\Sigma(v_n)^{-1}$\n",
    "  - Compute $F_{c_i}=H_{c_i}v_{c_i}$\n",
    "  - Compute $c_{c_i}=-\\log \\phi(v_{c_i};0,l_{n,c_i}\\Sigma(v_n))$\n",
    "- Reduce operation:\n",
    "  - Sum $\\sum_{i=1}^n c_{c_i}$, $\\sum_{i=1}^n F_{c_i}^T$, and $\\sum_{i=1}^n H_{c_i}$\n",
    "- Down operation:\n",
    "  - Compute $H_p=l_{p,n}^{-1}\\Sigma(v_p)^{-1}$\n",
    "  - Compute $F_p=H_pv_p$\n",
    "  - Compute $c_p=-\\log \\phi(v_n;0,l_{p,n}\\Sigma(v_p))$\n",
    "- Local update:\n",
    "  - Propose new $v_n’$ and $\\theta_n’$\n",
    "  - Compute acceptance ratio using the up and down results\n",
    "  - Accept or reject the proposal\n",
    "Note above that we only need to invert $\\Sigma(v_n,\\theta_n)$ one time to compute the up for all children.\n",
    "Nodes can be updated sequentially or in parallel with the same result as long as no node is the parent or child of another node that is being updated. This is achieved with a red/black node partition.\n",
    "\n",
    "In the implemented code below, the down operation is empty since we can reference $H_p$, $F_p$, $c_p$ of the parent node directly from the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperiax\n",
    "from jax.random import PRNGKey, split\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from hyperiax.execution import OrderedExecutor, UnorderedExecutor\n",
    "from hyperiax.models import UpLambdaReducer, DownLambda, UpLambda, UpdateLambdaReducer\n",
    "from hyperiax.models.functional import pass_up\n",
    "from hyperiax.tree.topology import symmetric_topology\n",
    "from hyperiax.tree import HypTree\n",
    "from hyperiax.plotting import plot_tree_text, plot_tree_2d_scatter\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup of the tree and the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topology = symmetric_topology(height=3, degree=2)\n",
    "plot_tree_text(topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimension\n",
    "d = 2\n",
    "\n",
    "tree = HypTree(topology)\n",
    "\n",
    "tree.add_property('edge_length', shape=(1,))\n",
    "tree.add_property('obs_var', shape=(1,))\n",
    "tree.add_property('noise', shape=(d,))\n",
    "tree.add_property('value', shape=(d,))\n",
    "tree.add_property('old_value', shape=(d,))\n",
    "tree.add_property('H', shape=(d,d))\n",
    "tree.add_property('F', shape=(d,))\n",
    "tree.add_property('C', shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, k1 = split(key)\n",
    "tree.data['value'] = jax.random.normal(key, shape=tree.data['value'].shape)\n",
    "tree.data['noise'] = jax.random.normal(k1, shape=tree.data['noise'].shape)\n",
    "tree.data['edge_length'] = 1/(tree.node_depths+1) # for testing\n",
    "tree.data['obs_var'] = tree.data['edge_length'] #jnp.ones_like(tree.data['obs_var'])*0.01\n",
    "repeated_eye = repeat(jnp.eye(d),'i j->n i j', n=len(tree))\n",
    "sigmas = tree.data['obs_var'][:,:,None]*repeated_eye\n",
    "tree.data['H'] = repeated_eye/tree.data['obs_var'][:,:,None]\n",
    "tree.data['F'] = jnp.einsum('nij,nj->ni', tree.data['H'], tree.data['value'])\n",
    "tree.data['C'] = -jax.scipy.stats.multivariate_normal.logpdf(tree.data['value'],jnp.zeros(d),sigmas)\n",
    "\n",
    "def sigma(value):\n",
    "    return repeat(jnp.eye(value.shape[-1]), 'i j -> n i j', n=value.shape[0])\n",
    "\n",
    "# only leaf values matter - rest can be assumed undefined despite having value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the up operation.\n",
    "\n",
    "- Up:\n",
    "  - Compute $H_{c_i}=l_{n,c_i}^{-1}\\Sigma(v_n)^{-1}$\n",
    "  - Compute $F_{c_i}=H_{c_i}v_{c_i}$\n",
    "  - Compute $c_{c_i}=-\\log \\phi(v_{c_i};0,l_{n,c_i}\\Sigma(v_n))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(value, edge_length, parent_value, params):\n",
    "    H = (1/edge_length[:,:,None])*jnp.linalg.inv(sigma(parent_value))\n",
    "    F = jnp.einsum('bij,bj->bi', H, value)\n",
    "    C = -jax.scipy.stats.multivariate_normal.logpdf(value,jnp.zeros(value.shape[-1]),sigma(value))\n",
    "    return {'H': H, 'F': F, 'C': C}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test it, we make an up model using sum reduce the propagated the leaf values all the way up to the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(child_H, child_F, child_C, params):\n",
    "    return {'H': child_H, 'F': child_F, 'C': child_C}\n",
    "\n",
    "model = UpLambdaReducer(up, transform, {'H': 'sum', 'F': 'sum', 'C': 'sum'})\n",
    "exe = OrderedExecutor(model)\n",
    "\n",
    "# test the executor\n",
    "exe.up(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the old $v_n$, and sample $v_n'$, we save the old values and can thus rerun the up pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, k1, k2 = split(key, num=3)\n",
    "tree.data['old_value'] = tree.data['value']\n",
    "tree.data['value'] = jax.random.normal(k1, shape=tree.data['value'].shape)\n",
    "\n",
    "exe.up(tree)\n",
    "\n",
    "# now we can decide which values to keep based on an acceptance ratio for example. MAKE SURE THIS IS RETURNED FROM THE UP !!\n",
    "# this will save the old value if the acceptance ratio is below a threshold\n",
    "tree.data['value'] = jnp.where(\n",
    "    tree.data['acceptance_ratio'] >= jax.random.uniform(k2, shape=tree.data['acceptance_ratio'].shape), \n",
    "    tree.data['value'], \n",
    "    tree.data['old_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, the down operation is empty since the values $H_p$, $F_p$, and $C_p$ of the parent node are accessible directly from the node to be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down():\n",
    "    ## not sure about equations and what to pass\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the local update.\n",
    "\n",
    "- Local update:\n",
    "  - Propose new $v_n’$ and $\\theta_n’$\n",
    "  - Compute acceptance ratio using the up and down results\n",
    "  - Accept or reject the proposal\n",
    "\n",
    "\\begin{align}\n",
    "& \\log p(v_n|v_p,v_{c_1},\\ldots,v_{c_n}) \\\\\n",
    "& = -\\big(c_p+\\sum_{i=1}^n c_{c_i}\\big)+\\big(F_p+\\sum_{i=1}^n F_{c_i}^T\\big)v_n-\\frac12 v_n^T\\big(H_p+\\sum_{i=1}^n H_{c_i}\\big)v_n\n",
    "-\\log p(v_{c_1},\\ldots,v_{c_n}|v_p)\n",
    "\\ .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(child_H, child_F, child_C, parent_C, parent_F, parent_H, value, leaf_mask, root_mask, **kwargs):\n",
    "    v = value ## propose instead\n",
    "    #theta = ...\n",
    "    # do this batched\n",
    "    #ll = - (parent_C + child_C) + (parent_F + child_F.T)-0.5*v.T*(parent_H + child_H)*v\n",
    "    accepted = jnp.ones(v.shape[0]) # properly calculate this\n",
    "\n",
    "    new_val = jnp.where(accepted[:,None], v, value)\n",
    "    return_val = jnp.where(leaf_mask[:,None], value, new_val)\n",
    "    return {'value': return_val}\n",
    "\n",
    "def up2(H,C,F, **kwargs):\n",
    "    return {'H': H, 'C': C, 'F': F}\n",
    "\n",
    "model = UpdateLambdaReducer(up_fn=up2, update_fn=update, reductions={'H': 'sum', 'F': 'sum', 'C': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exe = UnorderedExecutor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exe.update(tree, key=key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
