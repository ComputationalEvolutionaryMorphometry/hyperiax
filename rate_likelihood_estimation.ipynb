{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperiax\n",
    "from jax.random import PRNGKey, split\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from hyperiax.execution import OrderedExecutor\n",
    "from hyperiax.models import UpLambdaReducer, DownLambda, UpLambda\n",
    "from hyperiax.models.functional import pass_up\n",
    "from hyperiax.tree.topology import symmetric_topology\n",
    "from hyperiax.tree import HypTree\n",
    "from hyperiax.plotting import plot_tree_text, plot_tree_2d_scatter\n",
    "from matplotlib import pyplot as plt\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate likelihood estimation - rough draft\n",
    "\n",
    "In this notebook, we setup a rough draft of rate estimation of discerete variables \n",
    "\n",
    "This notebook follows work of Sergei T. \n",
    "\n",
    "The outline of this notebook comes in; \n",
    "\n",
    "    1. Creating the tree\n",
    "\n",
    "    2. Simulating discerete character from root to leafs.  (NOT implemented, i just draft some randomly)\n",
    "\n",
    "    3. One up pass using likelihood to restimate the the rate\n",
    "    \n",
    "    4. Optimizer to converge for correct estimation of the rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating the Tree\n",
    "\n",
    "First, we initialize a tree. This creates a tree with a chosen topology. This topology is a \"stupid\" class, in the sense that it contains no data - and only serves as a representation of the data we intend to work on.\n",
    "\n",
    "Setting `height=3` and `degree=2`, gives us a tree with 4 layers, where each node has `2` children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       None\n",
      "   ┌────┴────┐\n",
      "  None      None   \n",
      " ┌─┴──┐    ┌─┴──┐  \n",
      "None None None None\n"
     ]
    }
   ],
   "source": [
    "topology = symmetric_topology(height=2, degree=2)\n",
    "plot_tree_text(topology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      " ┌─┴─┐\n",
      " 1   2 \n",
      "┌┴┐ ┌┴┐\n",
      "3 4 5 6\n"
     ]
    }
   ],
   "source": [
    "tree = HypTree(topology,precompute_child_gathers=True)\n",
    "\n",
    "\n",
    "# Propreties of the tree\n",
    "\n",
    "# Branch/edge length, assumed to be constant one \n",
    "tree.add_property('edge_length', shape=(1,))\n",
    "tree.data[\"edge_length\"]  = jnp.array([1.0] * tree.size)\n",
    "\n",
    "\n",
    "for i,node in enumerate(tree.iter_topology_bfs()):\n",
    "   node.name = str(i)\n",
    "\n",
    "# Empty properties to fill out later \n",
    "\n",
    "# Chacters storing \n",
    "tree.add_property('value', shape=(2,))\n",
    "\n",
    "# plot tree again \n",
    "plot_tree_text(tree)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simulating node values\n",
    "\n",
    "Could be a markov chain, I will just draft some random values for now\n",
    "\n",
    "I have kept the strcutre of the code how to do it proberly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert some correct thing here. \n",
    "\n",
    "#@jax.jit\n",
    "#def down(noise, edge_length,parent_value, **args):\n",
    "#    # Insert \n",
    "#    return {'value':value}\n",
    "\n",
    "# Define root variable for our simulation \n",
    "#tree.data['value'] = tree.data['value'].at[0].set([0,0])\n",
    "\n",
    "# Execute the simulation on tree\n",
    "#downmodel = DownLambda(down_fn=down)\n",
    "#exe = OrderedExecutor(downmodel)\n",
    "#exe.down(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Draw random 0,1 in the size corresponding to the under here\n",
    "# Random sampler \n",
    "tree.data['value'] = tree.data['value'].at[tree.is_leaf].set(jax.random.bernoulli(key, p=0.5, shape=(sum(tree.is_leaf),2)))\n",
    "\n",
    "# Print over discerete variables \n",
    "print(tree.data['value'][tree.is_leaf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estimating the inner nodes\n",
    "\n",
    "Here we could mask our internal nodes \"value\" from our downsimulation, since we would only now the leaf values. \n",
    "We keep with this format, and therefore more our leaf data to another variable\n",
    "\n",
    "\n",
    "# Upward Pass Computation\n",
    "\n",
    "We estimate the upwards probability from the leafs to the root; \n",
    "\n",
    "$$\n",
    "p(parent)= p(t1)*p(t2) \n",
    "$$\n",
    "where $p(t1)=d1*exp(Q*t1)^T$, and t1 is the branch length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add_property('estimated_value', shape=(2,))\n",
    "leaf_data = tree.data['value'][tree.is_leaf]\n",
    "tree.data['estimated_value'] = tree.data['estimated_value'].at[tree.is_leaf].set(leaf_data)\n",
    "\n",
    "leaf_edgelength = tree.data['edge_length'][tree.is_leaf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate the probability in the leaf nodes, since we only have data in chacaters in the leafs, and upwards we multiply the edgelengths with probabilies (but I may have misunderstood that part)\n",
    "\n",
    "Manual estimating here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5  0.5]\n",
      " [ 1.  -1. ]]\n",
      "[0.60653067 1.6487212 ]\n",
      "all leafs\n",
      "[[1.6487212  0.60653067]\n",
      " [0.60653067 1.6487212 ]\n",
      " [0.60653067 1.6487212 ]\n",
      " [0.60653067 1.6487212 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define rate matrix, which is 2x2 as a function of input of alpha and beta\n",
    "# as a function \n",
    "Q_rate_matrix = lambda alpha, beta: jnp.array([[-alpha, alpha], [beta,-beta]])\n",
    "prob_estimation = lambda chacter,length : jnp.dot(chacter,jnp.exp(Q_rate_matrix(alpha,beta)*length))\n",
    "\n",
    "\n",
    "# Define alpha and beta\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "\n",
    "# Print the functions as a test\n",
    "print(Q_rate_matrix(0.5,1))\n",
    "print(prob_estimation(jnp.array([1,0]),1))\n",
    "\n",
    "# JUST A CHECK\n",
    "# Apply the functions on leaf data and estimate the probability\n",
    "leaf_data_prob = jax.vmap(prob_estimation, in_axes=0)(leaf_data,leaf_edgelength)\n",
    "print(\"all leafs\")\n",
    "print(leaf_data_prob)\n",
    "\n",
    "# assign the values in the leafs of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperiax implementation of up\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_function(child_estimated_value, child_edge_length, **kwargs):\n",
    "\n",
    "    # Do probability estimation \n",
    "    probs = jax.vmap(prob_estimation, in_axes=0)(child_estimated_value,child_edge_length)\n",
    "\n",
    "    # Multiply probabilities element-wise and reduce along the first axis\n",
    "    return {'estimated_value': jnp.prod(probs, axis=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upmodel = UpLambda(up_fn=probability_function)\n",
    "upmodelexe = OrderedExecutor(upmodel)\n",
    "res = upmodelexe.up(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_length': Array([1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " 'value': Array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=float32),\n",
       " 'estimated_value': Array([[10.610551  ,  5.086161  ],\n",
       "        [ 1.        ,  1.        ],\n",
       "        [ 0.36787945,  2.7182817 ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ]], dtype=float32)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the results\n",
    "tree.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual estimation of internal nodes\n",
      "[1. 1.]\n",
      "[0.36787945 2.7182817 ]\n",
      "internal nodes\n",
      "[[1.         1.        ]\n",
      " [0.36787945 2.7182817 ]]\n",
      "Root\n",
      "[10.610551  5.086161]\n"
     ]
    }
   ],
   "source": [
    "# Do manual computation to see the results is true \n",
    "\n",
    "leaf_data_prob = jax.vmap(prob_estimation, in_axes=0)(leaf_data,leaf_edgelength)\n",
    "\n",
    "# Check the first node at see it is the same\n",
    "print(\"manual estimation of internal nodes\")\n",
    "n3 = jnp.dot(leaf_data[0],jnp.exp(Q_rate_matrix(alpha,beta)*1))\n",
    "n4 = jnp.dot(leaf_data[1],jnp.exp(Q_rate_matrix(alpha,beta)*1))\n",
    "print(n3*n4)\n",
    "\n",
    "n5 = jnp.dot(leaf_data[2],jnp.exp(Q_rate_matrix(alpha,beta)*1))\n",
    "n6 = jnp.dot(leaf_data[3],jnp.exp(Q_rate_matrix(alpha,beta)*1))\n",
    "print(n5*n6)\n",
    "\n",
    "\n",
    "# Do reshape so it fits with the upwards pass\n",
    "leaf_data_prob = jax.vmap(prob_estimation, in_axes=0)(leaf_data,leaf_edgelength)\n",
    "leaf_data_prob = leaf_data_prob.reshape(2,2,2)\n",
    "\n",
    "# See these are the same\n",
    "midder = (jnp.prod(leaf_data_prob, axis=1))\n",
    "print(\"internal nodes\")\n",
    "print(midder)\n",
    "\n",
    "# and then do the last to see the value\n",
    "print(\"Root\") \n",
    "print(jnp.dot(jnp.array(midder[0]),jnp.exp(Q_rate_matrix(alpha,beta)*1))*jnp.dot(jnp.array(midder[1]),jnp.exp(Q_rate_matrix(alpha,beta)*1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do the optimization to estimate alpha/beta\n",
    "\n",
    "NOTE - I may not have recalled the likelihood function correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: -3.5667128562927246\n",
      "Iteration 10, Loss: -3.6169826984405518\n",
      "Iteration 20, Loss: -3.6757652759552\n",
      "Iteration 30, Loss: -3.743407726287842\n",
      "Iteration 40, Loss: -3.81976580619812\n",
      "Iteration 50, Loss: -3.9042000770568848\n",
      "Iteration 60, Loss: -3.995724678039551\n",
      "Iteration 70, Loss: -4.09318733215332\n",
      "Iteration 80, Loss: -4.195422172546387\n",
      "Iteration 90, Loss: -4.301346778869629\n",
      "Estimated alpha: 1.61799955368042\n",
      "Estimated beta: 1.6179993152618408\n",
      "Final negative log-likelihood: -4.410018444061279\n"
     ]
    }
   ],
   "source": [
    "# Use JAX's BFGS optimizer from scipy.optimize\n",
    "\n",
    "from jax.example_libraries import optimizers\n",
    "from jax import grad\n",
    "pi_root = [1,1]\n",
    "pred_root = tree.data[\"estimated_value\"][tree.is_root]\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "def negative_log_likelihood(params):\n",
    "    alpha, beta = params\n",
    "    Q = Q_rate_matrix(alpha, beta)\n",
    "    # note we do not include edge length since we are in the root\n",
    "    root_probs = jnp.exp(Q)\n",
    "    likelihood = jnp.sum(jnp.array(pi_root) * jnp.dot(pred_root, root_probs))\n",
    "\n",
    "    return -jnp.log(likelihood)\n",
    "\n",
    "\n",
    "# Initialize optimization parameters\n",
    "initial_params = jnp.array([0.5, 0.5])\n",
    "# Use JAX's optimizers for BFGS optimization\n",
    "from jax.example_libraries import optimizers\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define the optimization function\n",
    "@jax.jit\n",
    "def optimize_step(i, opt_state, pi_root, pred_root):\n",
    "    params = get_params(opt_state)\n",
    "    value, grads = jax.value_and_grad(negative_log_likelihood)(params)\n",
    "    return opt_update(i, grads, opt_state), value\n",
    "\n",
    "# Initialize the optimizer\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=0.01)\n",
    "opt_state = opt_init(initial_params)\n",
    "\n",
    "# Optimization loop\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    # Re-estimate the root value using upmodelexe.up\n",
    "    upmodelexe.up(tree)\n",
    "    pred_root = jnp.array(tree.data[\"estimated_value\"][tree.is_root])\n",
    "    \n",
    "    opt_state, value = optimize_step(i, opt_state, jnp.array(pi_root), pred_root)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {value}\")\n",
    "\n",
    "# Get the final optimized parameters\n",
    "final_params = get_params(opt_state)\n",
    "estimated_alpha, estimated_beta = final_params\n",
    "\n",
    "print(f\"Estimated alpha: {estimated_alpha}\")\n",
    "print(f\"Estimated beta: {estimated_beta}\")\n",
    "print(f\"Final negative log-likelihood: {negative_log_likelihood(final_params)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
